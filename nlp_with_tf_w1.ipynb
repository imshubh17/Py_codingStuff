{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lession 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubham\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=[\n",
    "    'i love dog',\n",
    "    'I, love my cat',\n",
    "    'what do i think ?'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'i': 2, 'love': 3, 'dog': 4, 'my': 5, 'cat': 6, 'what': 7, 'do': 8, 'think': 9}\n"
     ]
    }
   ],
   "source": [
    "t=Tokenizer(num_words=100,oov_token=\"<OOV>\")\n",
    "t.fit_on_texts(sentence)\n",
    "word_index=t.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3, 4], [2, 3, 5, 6], [7, 8, 2, 9]]\n"
     ]
    }
   ],
   "source": [
    "sequence=t.texts_to_sequences(sentence)\n",
    "print(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=[\n",
    "    'they love dog'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test sequence [[1, 3, 4]]\n"
     ]
    }
   ],
   "source": [
    "test_seq=t.texts_to_sequences(test_data)\n",
    "print(\"test sequence\",test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2 3 4]\n",
      " [2 3 5 6]\n",
      " [7 8 2 9]]\n"
     ]
    }
   ],
   "source": [
    "padded=pad_sequences(sequence)\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 3 4 0]\n",
      " [2 3 5 6]\n",
      " [7 8 2 9]]\n"
     ]
    }
   ],
   "source": [
    "padded_after=pad_sequences(sequence,padding='post')\n",
    "print(padded_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 3 4]\n",
      " [2 3 5]\n",
      " [7 8 2]]\n"
     ]
    }
   ],
   "source": [
    "padded_max=pad_sequences(sequence,padding='post',truncating='post',maxlen=3)\n",
    "print(padded_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# link to download dataset https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sarcasm.json\",\"r\") as f:\n",
    "    datastore=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=[]\n",
    "labels=[]\n",
    "url=[]\n",
    "for item in datastore:\n",
    "    sentences.append(item[\"headline\"])\n",
    "    labels.append(item[\"is_sarcastic\"])\n",
    "    url.append(item[\"article_link\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer(oov_token=\"<OOV>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index=tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences=tokenizer.texts_to_sequences(sentences)\n",
    "padded=pad_sequences(sequences,padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5 11 12  2 13 14 15 16  6 17 18 19  7 20 21 22 23]\n"
     ]
    }
   ],
   "source": [
    "print(padded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 17)\n"
     ]
    }
   ],
   "source": [
    "print(padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
